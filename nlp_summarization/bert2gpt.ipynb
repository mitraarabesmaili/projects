{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92514ad-e566-4f7b-8ecf-9f09a8aa5b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install hazm\n",
    "!pip install rouge\n",
    "! pip install -U accelerate\n",
    "! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2895c2-22eb-4a91-8852-f28cad9fa5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 06:21:27.939096: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-16 06:21:27.939612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-16 06:21:28.473865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 06:21:29.963700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 06:21:40.521366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from datasets import ClassLabel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments,AutoTokenizer\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import BertTokenizer\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012639f2-3fc8-43e6-aa9b-e40b72f1f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer=AutoTokenizer.from_pretrained(\"HooshvareLab/gpt2-fa\",\n",
    "    pad_token='<pad>',\n",
    "    bos_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>')\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d9c756-d2a5-464a-93dd-8db380760134",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ecd1f6-46a0-4342-bb30-d06225861784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d5c69e20b54a6bbbe0b0693343b3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pn_summary.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ddb8bd2d564a4aaac7da7992105cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f353d49bc8d4214bef642fe4da28d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pn_summary.zip:   0%|          | 0.00/89.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/datasets/pn_summary/49aa6a5fdb11244714f9bbe69517f2079ab934c9c565e272a977fbd8d2d404f7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pn_summary.zip%3B+filename%3D%22pn_summary.zip%22%3B&response-content-type=application%2Fzip&Expires=1734589323&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDU4OTMyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9wbl9zdW1tYXJ5LzQ5YWE2YTVmZGIxMTI0NDcxNGY5YmJlNjk1MTdmMjA3OWFiOTM0YzljNTY1ZTI3MmE5NzdmYmQ4ZDJkNDA0Zjc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=dLSFxEZ7FYVCG%7EcflcL17ofpXNIAEjTynSb7rGhhbB%7EWGhtJpmWHQNQlSXJojTczCsLDKB3Xz4oRh92nYnuJwf4IKzh8YBoq1gx3urj0Iw%7EXzEk2jBYQoYy1SfYOa3rH5EULYojoqH18OZucLliHSpkMmMQL2GTQzu7Ki70u2KCC2tTwPG4ryVh%7EVrift04qUyopCUscvEuyeP%7EpDID8GEEBkzfsb2ssJtIGqUO88qrlFr42wEOQ9FtHvQyVelo2gmojaZHw1G9V2xl6D7kTShiEQUsgv6Whz7SbFPc4QHuVz2ZSF4RuHJQfyt4IJOgPf7VDOkD4hwPOuqdfNsVLVQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903e5d89492a42a39fe91145e08a6ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pn_summary.zip:   0%|          | 0.00/89.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd943d7c96042fba5e3ae3b13827e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d74389b4d4905b2c88d37738c1ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62edb457215428da6d6e60f775fb428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = datasets.load_dataset(\n",
    "    \"pn_summary\", split=\"train\", download_mode=\"force_redownload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a6a82c-175f-4549-9c01-0499d182e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    for i in range(len(batch['article'])):\n",
    "      batch['article'][i]=normalizer.normalize(batch['article'][i])\n",
    "      batch['summary'][i]=normalizer.normalize(batch['summary'][i])\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "    )\n",
    "    outputs = gpt_tokenizer(\n",
    "        batch[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=decoder_max_length,\n",
    "    )\n",
    "    outputs[\"input_ids\"] = [\n",
    "        [(label if label != gpt_tokenizer.pad_token_id else -100) for label in l]\n",
    "        for l in outputs[\"input_ids\"]\n",
    "    ]\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    # batch[\"labels\"] = [\n",
    "    #     [1 if token == gpt_tokenizer.pad_token_id else token for token in labels]\n",
    "    #     for labels in batch[\"labels\"]\n",
    "    # ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d586c307-2c65-4b7e-bada-40dd5f0f069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1d60d67-8ec4-4885-a1e3-360ad79b764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id for decoding\n",
    "    labels_ids[labels_ids == -100] = gpt_tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = gpt_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = gpt_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.get_scores(decoded_preds, decoded_labels,avg=True)\n",
    "    print(result)\n",
    "    with open(\"result.txt\",\"a\") as handler:\n",
    "        handler.write(f'rouge-1 - recall: {result[\"rouge-1\"][\"r\"]} - precision: {result[\"rouge-1\"][\"p\"]} - fscore: {result[\"rouge-1\"][\"f\"]}\\n')\n",
    "        handler.write(f'rouge-2 - recall: {result[\"rouge-2\"][\"r\"]} - precision: {result[\"rouge-2\"][\"p\"]} - fscore: {result[\"rouge-2\"][\"f\"]}\\n')\n",
    "        handler.write(f'rouge-l - recall: {result[\"rouge-l\"][\"r\"]} - precision: {result[\"rouge-l\"][\"p\"]} - fscore: {result[\"rouge-l\"][\"f\"]}\\n\\n')\n",
    "    # Extract F-measure for each ROUGE score\n",
    "    rouge_result = {\n",
    "        \"rouge1\": result[\"rouge-1\"][\"f\"],\n",
    "        \"rouge2\": result[\"rouge-2\"][\"f\"],\n",
    "        \"rougeL\": result[\"rouge-l\"][\"f\"],\n",
    "    }\n",
    "\n",
    "    return rouge_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fac2d221-7763-49cd-876e-7c0f3141c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "#train_data = train_data.select(range(1000))\n",
    "# batch_size = 16\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e1cf3e-4353-46da-a3f2-f5ee36d5eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73953bbe02a448eaf08d242c519490b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(\n",
    "        process_data_to_model_inputs,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=['id', 'title', 'article', 'summary', 'category', 'categories', 'network', 'link'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa834143-c5b9-40c0-9b59-a5dabdc2cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f06781-2701-4b88-812b-2eeaf403010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = datasets.load_dataset(\n",
    "        \"pn_summary\", split=\"validation\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3e2bc7-c984-4d08-bfed-563655f1a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data = val_data.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c65877-9874-41c6-bdac-88af36591b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be41d735fc7477d8435156a0f586aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data = val_data.map(\n",
    "        process_data_to_model_inputs,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=['id', 'title', 'article', 'summary', 'category', 'categories', 'network', 'link'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0595210f-f235-4476-9964-a459e25a2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c214b9-fc26-4e0b-8674-cad607a6af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at HooshvareLab/gpt2-fa and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert2gpt = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        \"HooshvareLab/bert-base-parsbert-uncased\", \"HooshvareLab/gpt2-fa\"\n",
    "    )\n",
    "bert2gpt.save_pretrained(\"bert2bert\")\n",
    "bert2gpt = EncoderDecoderModel.from_pretrained(\"bert2bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02141e7e-8741-4b2d-b7db-5bd707fe8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2gpt.config.decoder_start_token_id = gpt_tokenizer.bos_token_id\n",
    "bert2gpt.config.pad_token_id = gpt_tokenizer.pad_token_id\n",
    "bert2gpt.config.vocab_size = bert2gpt.config.decoder.vocab_size\n",
    "bert2gpt.generation_config.pad_token_id = gpt_tokenizer.eos_token_id\n",
    "bert2gpt.config.max_length = 128\n",
    "bert2gpt.config.min_length = 0\n",
    "bert2gpt.config.no_repeat_ngram_size = 2\n",
    "bert2gpt.config.early_stopping = True\n",
    "bert2gpt.config.length_penalty = 2.0\n",
    "bert2gpt.config.num_beams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383b8afd-3c5d-4cd1-a943-7e2c7e51be6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpt_tokenizer.eos_token_id\n",
    "bert2gpt.generation_config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d65c9fe-3e90-435f-b421-510334349622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        output_dir=\"bert2gpt_model\",\n",
    "        logging_steps=2,\n",
    "        fp16=True,\n",
    "        save_steps=10000,\n",
    "        eval_steps=10000,\n",
    "        num_train_epochs=5,\n",
    "        report_to=\"none\",\n",
    "        warmup_steps=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b4c0-09b1-43ac-8c6d-1d897a338716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_301/4257497199.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:649: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3588' max='102530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3588/102530 09:59 < 4:35:38, 5.98 it/s, Epoch 0.17/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=bert2gpt,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "    )\n",
    "trainer.train()\n",
    "handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66715847-08fe-48a5-955e-b2ef956fb68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed1f3b-1ef9-461d-b69e-4530d6645af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa644fc-fbb6-4ef2-be61-f52d3e072b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0318ab2-67c4-4898-a248-5d2d16c12e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169349e-b496-4002-bf6b-c1a6f7f9a07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
